{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAPT: Test-Time Adversarial Prompt Tuning\n",
    "\n",
    "This notebook replicates results from the TAPT paper (CVPR 2025).\n",
    "\n",
    "**Paper**: [TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models](https://arxiv.org/abs/2411.13136)\n",
    "\n",
    "## Workflow\n",
    "1. **Setup Environment** - Install dependencies\n",
    "2. **Mount Google Drive** - Access datasets\n",
    "3. **Clone Repository** - Get the code\n",
    "4. **Option A**: Use pre-trained weights (if available)\n",
    "5. **Option B**: Train AdvIVLP weights on ImageNet (requires ImageNet dataset)\n",
    "6. **Run TAPT Evaluation** - Evaluate on DTD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clone Repository\n",
    "\n",
    "**TODO**: Replace `YOUR_GITHUB_USERNAME` with your actual GitHub username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository (replace with your GitHub repo URL)\n",
    "%cd /content\n",
    "!git clone https://github.com/YOUR_GITHUB_USERNAME/Baseline.git\n",
    "%cd Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dassl.pytorch\n",
    "%cd /content/Baseline/Dassl.pytorch\n",
    "!pip install -r requirements.txt\n",
    "!python setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CLIP\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies\n",
    "!pip install ftfy regex tqdm scipy scikit-learn tabulate yacs gdown future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add paths to Python path\n",
    "sys.path.insert(0, '/content/Baseline/TAPT')\n",
    "sys.path.insert(0, '/content/Baseline/Dassl.pytorch')\n",
    "sys.path.insert(0, '/content/Baseline/Multimodal-Adversarial-Prompt-Tuning')\n",
    "\n",
    "# Set DATA path\n",
    "os.environ['DATA'] = '/content/drive/MyDrive/datasets'\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"DATA path: {os.environ['DATA']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Dataset\n",
    "\n",
    "Make sure your DTD dataset is structured correctly:\n",
    "```\n",
    "/content/drive/MyDrive/datasets/\n",
    "‚îî‚îÄ‚îÄ dtd/\n",
    "    ‚îú‚îÄ‚îÄ images/\n",
    "    ‚îú‚îÄ‚îÄ imdb/\n",
    "    ‚îú‚îÄ‚îÄ labels/\n",
    "    ‚îî‚îÄ‚îÄ split_zhou_DescribableTextures.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify DTD dataset structure\n",
    "import os\n",
    "\n",
    "dtd_path = '/content/drive/MyDrive/datasets/dtd'\n",
    "\n",
    "if os.path.exists(dtd_path):\n",
    "    print(\"DTD dataset found!\")\n",
    "    print(\"Contents:\")\n",
    "    for item in os.listdir(dtd_path):\n",
    "        full_path = os.path.join(dtd_path, item)\n",
    "        if os.path.isdir(full_path):\n",
    "            print(f\"  üìÅ {item}/\")\n",
    "        else:\n",
    "            print(f\"  üìÑ {item}\")\n",
    "    \n",
    "    # Check for split file\n",
    "    split_file = os.path.join(dtd_path, 'split_zhou_DescribableTextures.json')\n",
    "    if os.path.exists(split_file):\n",
    "        print(\"\\n‚úÖ Split file found!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå WARNING: split_zhou_DescribableTextures.json not found!\")\n",
    "        print(\"Download from: https://drive.google.com/file/d/1u3_QfB467jqHgNXC00UIzbLZRQCg2S7x/view\")\n",
    "else:\n",
    "    print(f\"‚ùå DTD dataset not found at {dtd_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports\n",
    "try:\n",
    "    import torch\n",
    "    import clip\n",
    "    from dassl.config import get_cfg_default\n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "    print(f\"   - PyTorch: {torch.__version__}\")\n",
    "    print(f\"   - CLIP models available: {clip.available_models()}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Option A: Using Pre-trained Weights (if available)\n",
    "\n",
    "If you have pre-trained AdvIVLP weights, upload them to your Google Drive and update the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell if you don't have pre-trained weights\n",
    "# This is just to check if weights exist\n",
    "\n",
    "weights_path = '/content/Baseline/output/train/imagenet/AdvIVLP/vit_b16_c2_ep100_batch32_2+2ctx_9depth_16shots/seed1'\n",
    "\n",
    "if os.path.exists(weights_path):\n",
    "    print(f\"‚úÖ Pre-trained weights found at: {weights_path}\")\n",
    "    print(\"Contents:\")\n",
    "    for item in os.listdir(weights_path):\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"‚ùå Pre-trained weights not found.\")\n",
    "    print(\"You need to either:\")\n",
    "    print(\"  1. Train AdvIVLP weights (Option B below)\")\n",
    "    print(\"  2. Contact authors for pre-trained weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Option B: Train AdvIVLP Weights (Requires ImageNet)\n",
    "\n",
    "‚ö†Ô∏è **WARNING**: This requires the ImageNet dataset (~150GB) and significant training time.\n",
    "\n",
    "Training settings:\n",
    "- 16-shot samples from ImageNet\n",
    "- 100 epochs\n",
    "- Batch size: 32 (may need to reduce for T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ImageNet is available\n",
    "imagenet_path = '/content/drive/MyDrive/datasets/imagenet'\n",
    "\n",
    "if os.path.exists(imagenet_path):\n",
    "    print(f\"‚úÖ ImageNet found at: {imagenet_path}\")\n",
    "    print(\"You can proceed with training.\")\n",
    "else:\n",
    "    print(\"‚ùå ImageNet not found.\")\n",
    "    print(\"Training requires ImageNet dataset.\")\n",
    "    print(\"See DATASETS.md for download instructions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AdvIVLP (only run if ImageNet is available)\n",
    "# This will take several hours\n",
    "\n",
    "%cd /content/Baseline/Multimodal-Adversarial-Prompt-Tuning\n",
    "\n",
    "!python train.py \\\n",
    "    --root /content/drive/MyDrive/datasets \\\n",
    "    --seed 1 \\\n",
    "    --trainer AdvIVLP \\\n",
    "    --dataset-config-file configs/datasets/imagenet.yaml \\\n",
    "    --config-file configs/trainers/AdvIVLP/vit_b16_c2_ep100_batch32_2+2ctx_9depth.yaml \\\n",
    "    --output-dir /content/Baseline/output/train/imagenet/AdvIVLP/vit_b16_c2_ep100_batch32_2+2ctx_9depth_16shots/seed1 \\\n",
    "    DATASET.NUM_SHOTS 16 \\\n",
    "    DATALOADER.TRAIN_X.BATCH_SIZE 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 7: Run TAPT Evaluation on DTD (Clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TAPT-VLI on DTD (Clean evaluation)\n",
    "# Make sure you have pre-trained weights before running this\n",
    "\n",
    "%cd /content/Baseline/TAPT\n",
    "\n",
    "!python train.py \\\n",
    "    --root /content/drive/MyDrive/datasets \\\n",
    "    --seed 1 \\\n",
    "    --trainer TAPTVLI \\\n",
    "    --dataset-config-file configs/datasets/dtd.yaml \\\n",
    "    --config-file configs/trainers/TAPTVLI/TAPT_vit_b16_c2_ep100_batch32_2ctx_9depth_l1_cross_datasets_step1_clean.yaml \\\n",
    "    --output-dir output/TAPTVLI/clean/dtd/seed1/100 \\\n",
    "    --model-dir /content/Baseline/output/train/imagenet/AdvIVLP/vit_b16_c2_ep100_batch32_2+2ctx_9depth_16shots/seed1 \\\n",
    "    --load-epoch 100 \\\n",
    "    --tapt \\\n",
    "    DATASET.NUM_SHOTS 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View evaluation results\n",
    "import os\n",
    "\n",
    "results_dir = '/content/Baseline/TAPT/output/TAPTVLI/clean/dtd/seed1/100'\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    print(f\"Results directory: {results_dir}\")\n",
    "    print(\"\\nContents:\")\n",
    "    for item in os.listdir(results_dir):\n",
    "        print(f\"  - {item}\")\n",
    "    \n",
    "    # Try to read log file\n",
    "    log_file = os.path.join(results_dir, 'log.txt')\n",
    "    if os.path.exists(log_file):\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"LOG FILE (last 30 lines):\")\n",
    "        print(\"=\"*50)\n",
    "        with open(log_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines[-30:]:\n",
    "                print(line.rstrip())\n",
    "else:\n",
    "    print(f\"Results not found at {results_dir}\")\n",
    "    print(\"Make sure to run the evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Out of Memory (OOM)**: Reduce batch size in the config file or command line\n",
    "   ```\n",
    "   DATALOADER.TRAIN_X.BATCH_SIZE 2\n",
    "   DATALOADER.TEST.BATCH_SIZE 50\n",
    "   ```\n",
    "\n",
    "2. **Missing split file**: Download `split_zhou_DescribableTextures.json` from:\n",
    "   https://drive.google.com/file/d/1u3_QfB467jqHgNXC00UIzbLZRQCg2S7x/view\n",
    "\n",
    "3. **Import errors**: Make sure to run all setup cells in order\n",
    "\n",
    "4. **Dataset not found**: Check the path to your Google Drive datasets folder"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
